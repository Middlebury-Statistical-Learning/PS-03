1. crime recidivism
Even though questions about race and ethnicity might be avoid before calculating the score for crime recidivism, these things are implied through other questions like “what’s your postal code?” and “have any other people in your family committed crime”. There is more interaction with the police in areas where people of color live because of poverty and several other factors, so it is more likely that people who have had family members deal with the police are from those areas. So, the algorithms that don’t seem racist, will turn out to be racist in so many cases. People who have been jailed before will be more likely to be jailed again, because they end up with low scores because of these algorithms. 


2. the thought experiment of hiring in tech firms
Tech hiring firms use different machine learning algorithms to look for candidates by scraping their resume and performing these algorithms on them. These algorithms look at people who have succeeded at those companies in the past and find people who are more likely to succeed. Women throughout history haven’t been given good opportunities, so it’s likely that it’s men who’ve succeeded at these companies in the past. Since the algorithm learns from past success and tries to predict future success, it’s likely that it will ignore most women’s resumes. This creates a huge gender imbalance in the tech industry.


3. teacher evaluations
While doing teacher evaluations, scoring was done in the past where a teacher was given a score by counting the number of students who were proficient in their course by the end of the class. This didn’t really work, because proficiency might be a proxy for poverty (poorer students who haven’t had the resources and opportunities might not have a solid base and might not get proficient in the course). Because of this, ended up punishing good teachers who happen to teach students in area of poverty. Nowadays, algorithms that look at these past models, and predict student performance are used, and each student is supposed to have a different prediction based on various factors. If a student performs better than expected, it’s good for the teacher’s score. The problem here is that since initial models are bad, we end up punching teachers for the factors that affected these models. We might be also punishing teachers for noise in the data. 
