Mathematical models reinforce inequality in the context of crime recidivism because they are unjustly biased against offenders of impoverished and minority backgrounds.  These models utilize crime and police data to predict how likely an offender is to commit another crime upon release from prison, and using that value determine how long the offender should be sentenced to prison.  However, the crime and police data being used to train the model is biased against minority and poor individuals due to racial profiling that places minority neighborhoods under stricter police scrutiny as well as the broad definition of crime that causes the training data to include a high proportion of petty nuisance crimes for which poor and non-white individuals are more likely to get in trouble for.  In addition, the questionnaire that offenders are asked to answer with this system is loaded with questions that are proxy questions for class and race.  Thus, due to the biased incoming dataset and the targeted questions on the questionnaire, the model is trained to predict that poor and non-white offenders are more likely to commit crimes and therefore when offenders state that they come from poor and/or minority communities, they are unfairly punished with longer sentences due to their socioeconomic status and/or race.

The model described in the thought experiment about hiring in tech firms perpetrates gender inequality because the algorithm doesn’t consider the bias in the training data generated by the already present gender disparity in the tech sector.  This model predicts whether an applicant would be 
successful within the company based on the demographics and patterns of current employees that are deemed “successful” within the company.  However, the issue of this model lies in the skewed incoming training data which contain few female employees due to the generally lower proportion of women in the tech industry and even fewer “successful” female employees due to the sexist work culture and promotion practices.  Thus, the algorithm will not select women from the applicant pool because it will have been trained to predict men as significantly more likely to be successful than women, and thus gender inequality in the hiring practices of the industry will continue to be reinforced.

The teacher evaluation model is problematic because the model uses student scores on standardized tests as a metric for grading instructors but the model is unable to account for all the factors that go into student performances.  For example, the model attempts to predict how well a student should perform on standardized tests at the end of the year based on a number of factors including how well they did in past years as well as the district they’re in and their family’s income and class level.  Then, using that prediction, the teacher is scored based on how close the student gets to attaining that expected score on the standardized tests.  However, this model is very inaccurate as there are so many unaccounted for factors that go into student performance, and thus the teacher’s grade may be incredibly skewed due to the unsystematic noise that goes into the model.  Thus, there is commonly an inconsistency of scoring for individual teachers on both a year-to-year basis as well as within the same school year if a teacher is scored upon multiple classes.  Therefore, the model’s inaccuracy and inconsistency makes it a meaningless evaluator of a teacher’s instruction quality. 

