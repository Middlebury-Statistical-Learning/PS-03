	A key question when using data to draw inferences is the quality of the data itself. Does the data actually capture what we think it captures? One problem that can arise in data is systematic bias. An example of this is algorithms used to predict recidivism rates. The data itself may be systematically biased, so the predictions based on it will also be biased. For instance, imagine two neighborhoods. One is predominately made up of minorities. The other is made up predominately of whites. Assume that the amount of crime that actually occurs in these neighborhoods is the same. But police spend much more time in the minority neighborhood (because of racial profiling), so much more crime gets reported in this neighborhood. Looking at the data, a naive researcher would conclude that minorities are much more likely to commit crimes. But the reality is that minorities are just more likely to get caught committing a crime. When biased data of this kind is used to predict recividism rates, the algorithm will predict that a person from a racial minority is more likely to commit a crime. The algorithm wwould suggest punishing this person more to prevent recidivism. In short, racial bias in policing creates racial bias in the data which results in racial bias in sentencing.  
 	Another example of biased data leading to biased predictions is hiring in tech companies. Say a tech company wants to develop an algorithm to help it hire new engineers. It collects data on its current top engineers and concludes that top engineers are all young, male, Stanford graduates. The issue is that their sample of engineers itself may be biased. The firm may have been discriminating during the hiring process and only hired engineers that fit this profile. Their sample is not representative of top engineers. It is representative of what the firm thinks are top engineers, and this notion may be based on unfounded profiling. If the firm trusts the algorithm entirely, it would continue to hire young, male Stanford graduates. The algorithm would reinforce the firm's preexisting biases and not actually help the firm find new, talented engineers. 
	A seperate issue discussed in the podcast is transparency. Algorithms can be "black boxes." No one, apart from the algorithm's creators, can look inside and see exactly how it's constructed. This was the case with the teacher evaluation algorithm in New York City. The scores generated by the algorithm were being used to fire teachers, but the teachers nor the public could learn how these scores were being generated. This is unfair to educators who are being evaluated by some unknown standards. Moreover, if educators don't know how they are being evaluated, how can they be expected to improve? Another issue with the lack of transparency is that the algorithm may be bad. O'Neil gives the example of a renowned teacher who went from a 6 out of 100 rating one year to a 93 rating the next year. It is unlikely a teacher's performance could vary this much, so there must have been something wrong with the algorithm. Without allowing others to examine it, however, the mistakes will not be corrected until they cause enough damage to force the creators to go back and fix them. If we have code review in this introductory statistical learning course, surely there should be code review for algorithms that determine which teachers get to keep their jobs. 


