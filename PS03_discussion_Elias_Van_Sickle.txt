Weapons of Math Destruction


In her interview on Econ Talk, Cathy O’Neil, author of Weapons of Math Destruction, discusses the prevalence and implications of problematic algorithms.  Such algorithms, she says, have the following three characteristics: they are widespread, secret, and destructive.  That is to say that these algorithms are they are deployed on many people to make important decisions about their lives, the true nature of the formula or algorithm is often obscured from view, and they can lead to pernicious feedback loops that are destructive to society as a whole.  


Recidivism risk scores rank highly among O’Neils concerns.  Recidivism, or the tendency of a convicted criminal to reoffend, is being predicted by algorithms that produce a score, which is given to a judge, and can inform the individual’s sentencing.  Cathy states that there is evidence to suggest that higher the recidivism scores, cause judges to give longer sentences.  Longer sentences, in turn, isolate the individual from society for longer making it harder to reintegrate and thus more likely to end up back in jail.  Additionally, certain components that are baked into these recidivism models do not necessarily reflect the quality of character of the individual in question or the likelihood that that specific person will continue to break the law, but rather systemic injustices in the form of proxies for race or class such as zip code.  Biased data coming in translates to biased results.  In this way, these algorithms can perpetuate systemic injustices.  Rather than accuracy, O’Neil argues the data science community should be chiefly concerned with causality.  


The next major topic of conversation stemmed from the following thought experiment.  Imagine a tech company that wants to hire engineers.  To facilitate this process, the company employs an algorithm to help sort through resumes.  In general, for this algorithm to be able to do its job, it must have a defined dataset and a model of success, in this case an individual who was hired and succeeded at the company.  O’Neil suggests that such an algorithm might totally reject women from the hiring pool.  She argues that while gender likely has little to do with software engineering talent, the algorithm’s models for success does reflect the larger sexist culture in the tech industry.


Finally, the teacher value model is discussed.  In an attempt to improve testing results and teacher efficacy, certain algorithms have been employed to evaluate teacher’s performance in the classroom.  The first thing to note is the underlying premise that education can be fixed by getting rid of bad teachers.  This certainly might be part of the solution but not the entire story.  Additionally, such models often hinge on students’ performances on standardized test scores.  To reduce good teaching to an improved test score is to not account for any of the nuance that makes a good teacher a good teacher.  Furthermore, because such test scores are highly correlated to poverty, the teachers that are labeled “bad” are often those who are teaching poor kids.  Finally, the year over year results for the same teachers demonstrated that the model was incredibly inconsistent.  Rather than a linear distribution it more closely resembled a uniform distribution with only a 24% correlation.  




It is not data analysis or machine learning algorithms themselves that constitute the problem.  Instead, it is the absence of scrutiny of models and the real world implications of these biased models that are the real issue.  O’Neill believes the first step toward mitigating these pernicious feedback loops and harmful outcomes is increased awareness through data science education that incorporates ethics into the equation.