In her podcast, Data Scientist Cathy O’Neil argues how mathematical models actually help reinforce inequalities through biased datasets and predictions. Within crime recidivism, it measures the likelihood of one’s return to prison based on a score they receive. Higher scores of recidivism are shown to correlate with a higher likelihood of returning to prison, and higher scores tend to get sentenced longer. However, what’s also shown in these models are two problematic variables: biased interactions with the police force that would garner assumptions, and questions based on an individual’s race and class through proxies. Scoring on a scale of low, medium, and high risk of recidivism, O’Neil claims that the models, for example, predict higher recidivism towards black defendants than they actually are.

Her case, however, is also prominently seen in the hiring process in technology firms. While created with good intention—possibly to maintain fairness and objectiveness—these models used in the hiring process sort through resumes and applications, without a human processing the given information, which may lead to mechanical assertions that are highly inaccurate. In her talk, she mentions how these algorithms used are reinforcing sexism in that female applicants were not accepted to a company. This, she argues, is the way in which algorithms are picking out patterns without the due notion of morale and how the analyses of these data may not be enough. If, for instance, the trend of the company’s hiring process is more male dominated, then this pattern limits anything that isn’t considered male.  While these automatic systems are not the only test or measurement for the hiring process, putting applicants on the top of the pile or landing an interview becomes a challenge.

Moreover, O’Neil recognizes this process in even teacher evaluations. Described as “crude” and “flawed,” teacher evaluations are seen as a way to filter out bad teachers. In these assessment models, however, they don’t recognize that performance on exams is based largely in part with poverty—meaning models are punishing teachers with poor students and the teachers are held accountable by the noise in the data.

These practices that are held by such models and ran by mathematical equations are studying out desires, movements, and patterns, and taking the data to formulate predictions. But the accuracies of the models, even when trained, lack sentience. And inaccurately, by lacking sentience and only looking for “patterns”, the results are no longer unbiased.

 