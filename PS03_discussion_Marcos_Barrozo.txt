A Comment on Cathy O’Neil’s Econ Talk interview:
	I believe a recurrent theme in this podcast episode was how machine learning models reproduce the behavioral and institutional biases inherent in our own decision-making processes. Because we train models using preexisting data, any resulting predictions will identify and repeat those patterns, no matter how immoral they might be. 

	In the context of crime, the models which predict recidivism use variables which are highly correlated with race. Given how racially biased the U.S. justice system has been for many years, the statistical models pick up that bias and predict greater recidivism for certain racial groups. That, in turn, only deepens the racial inequalities already present in the system. It is a similar situation in models for tech firms. Historically, women have been a very small proportion of employees in the sector. That is a result of many society-induced constraints, which don’t imply women are any less capable of success in tech jobs. However, statistical models only pick up the patterns, and again they reproduce the bias present in the past. Basically, machines are not (yet) made to question and criticize our society – they merely repeat patterns.

	The teacher evaluations story is slightly different. It has to do with how grades can be a poor measure of student performance, even if one considers them as a change on a baseline grade, or a school average. Evaluating teachers based on their students’ grades will inherit this measurement error, and as a result teachers can be rewarded or lose their jobs based on a terribly inaccurate predictor of performance. The lesson O’Neil draws from these examples is clear: one needs to be careful when using data-driven models, as they carry the same biases and inaccuracies which are inherent to our daily lives.
