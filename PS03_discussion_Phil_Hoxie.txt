Phil Hoxie 
Weapons of Math Destruction 
3/10/17

How objective models create feedback loops of inequality (and other unfair outcomes)

Crime recidivism models attempt to predict a convict’s likely hood of returning to prison. The model produces a score, which the judge will use in sentencing. Those who are labeled “high risk” get longer sentences. The problem is that the variables that go into this model are mostly proxy variables for poverty. This includes zip code, police encounters, and other factors which go into a logistic model to predict the likelihood of returning to prison. However, the goal is to be predictive, and this reinforces a feedback loop of poor people getting caught in the criminal justice system. Jail drains resources and economic opportunities which hurt individuals. Poor individuals are more likely to have minor police interactions and meet other criteria which make them “high risk”. This presents a biased data set. Therefore, the increased sentences resulting from this “objective” model is a self-fulfilling prophecy. Poor people go to jail, become poorer because they are in jail for longer, and then commit small crimes which send them back to prison.  

Tech hiring firms which use resume sorting models are another weapon of math destruction. Depending on how success is defined, an algorithm that looks for people with matching characteristics to previous “successes” will entrench historical mistakes. The example that was given is that a firm which discriminates against women in the promotion process may make an algorithm that rejects all female candidates because there are no female “successes” already at the firm. It entrenches historical practices. 

The teacher value added model is the final weapon. The model uses the difference in expected test scores versus actual test scores to evaluate teacher performance. The problem is that the data is incredibly noisy because a number of factors go into scores that the teacher has little to no control over. One math teacher exemplified the excess of noise in the data by looking at teachers who taught two different years. He found that there was an “almost normal” distribution of scores when plotting one class against the other for each teacher, instead of the expected 45 degree line. In an extreme case, a past teacher could cheat to inflate her students’ scores, and if next year’s expected scores are based off of the previous years’ scores, then second teacher will get a poor grade. This often leads to good teacher getting poor scores and sometimes getting fired. Not to mention the fact that test scores are a poor measure of learning. 
