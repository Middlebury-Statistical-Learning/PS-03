Explain in three paragraphs Cathy O’Neil’s argument of how supposedly objective mathematical/algorithmic models reinforce inequality in the context of 1) crime recidivism, 2) the thought experiment of hiring in tech firms and 3) teacher evaluations. Cathy O’Neil’s arguments for data as a weapon were largely centered on data going into a model withholding important historical circumstances. Machine learning algorithms cannot account for embedded inequalities due to existing systems of inequity - and hence according to O’neil - reality is not truly reflected in our data and data models. These biased data models, if used for decision making, cyclically reinforce an unjust system. Her arguments call for machine learning algorithms to include a human component. O’neil talks about how a model could bias the workforce to only include males in the tech industry by excluding the historical forces coming into play. If this model tries to pick out the candidates that succeed best, it would include mostly males in tech because those are the people that tend to represent the largest pool and succeed the most. The model does not know that societal pressures have pushed females to a) never join tech (smaller pool) and b) be pushed out of tech once they start. O’neil talks about how because an already established male workforce in the industry, it could be harder for females to feel comfortable in the industry and hence never be offered promotions. In another scenario she offered, she suggested they could be presumably be pushed out due to sexual harassment by their male counterparts. These instances of human behavior and culture are not accounted for in our datasets. If datasets and models do not account for the bias in industries - then they are implicitly biased models themselves.
