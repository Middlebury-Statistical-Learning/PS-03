Nina SonnebornMATH 218PS03 Cathy O’Neil PodcastIn her podcast, Cathy O’Neil discusses how machine learning algorithms and other prediction algorithms reinforce inequality. She calls these models, the details of which are often kept secret, are a “decision making process without an auditing process.” O’Neil asks us to question whether it is appropriate to make decisions based on predictions, if factors that go into the prediction aren’t necessarily causal or controllable. She focuses on three specific models with damaging effects.Crime recidivismMany states us prediction models to determine the probability that a criminal will return to prison after release. These predictions are often used in court- those with a calculated high probability of recidivism may get longer sentences. Well, then people who have longer sentences may have more trouble reintegrating into society and thus end up back in jail – it’s a feedback loop. The problem is that this prediction model perpetuates the problem – it’s calculated (although indirectly) based on race and socioeconomic status, plus data from police reporting. These predictors are accessed by proxy (ZIP code, questions like “Did you come from a high crime area?” or “Do any family member have crime history?”), and the police data is already uneven (and in my conclusion, racially biased).The thought experiment of hiring in tech firms O’Neil says that with predictions, we hope that the truth in embedded in historical practices, but “truth is only embedded in historical practices if historical practices are perfect.” She gives the hypothetical example of a tech company using machine learning to sort through applicant resumes. They want to find people who look like the people who succeeded in the past, but in doing this they could exclude women, simply because they don’t look like the historic data.Teacher evaluationsO’Neil discusses the crude assessment algorithm used for teacher assessments. She says it used to be plain student proficiency, which is highly correlated to poverty. They now use a derivative model that depends on a model that predicts what a student should get and score based on the average error term. However, the model that predicts student scores is not that accurate. While the assessment scores are public, the source code is kept secret from even the Dept. of Education. But consistency between the same teacher across multiple classes in the same year is only 24%. Teachers are then sometimes fired based on these scores.