This podcast was about the overlooked societal factors associated with using big data to make predictions using machine learning.
	Crime recidivism models are hella racist. Although race is not a variable that is inputted into the equation, zip code and socio-economic status are used as proxies for race. If a police force were looking at the places where criminals were most likely to be, using this algorithm, they would spend more time in predominantly African-American neighborhoods. This creates a positive feedback cycle, as increased police presence would inflate the number of ‘criminals’ (I put that in quotes because a lot of laws that send people to jail are stupid rules that were created through a system of inequality (it’s infuriating why marijuana is illegal - http://www.attn.com/stories/2116/reason-marijuana-illegal-united-states)) arrested in certain areas, so there would be an increased policed presence. So, using the algorithm to determine whether someone is likely to return to prison makes them more likely to return to prison.
	What’s ironic about the thought experiment is that it isn’t a thought experiment, it actually happens. But, O’Neil lays out a scenario where a tech company creates an algorithm to determine who would be successful at the company. They say someone is successful if they have been at the company for longer than three years and have been promoted twice. They make an accurate model for the current company makeup (their train data), but when they apply the algorithm to the applicant pool (their test data), no women are selected. This is because institutionalized sexism made women employees less successful and less ‘promotable’ at the company. If the company made their hiring decisions off the model, or let the model heavily influence their decision, women and other minority groups would be severely disadvantaged. A potential solution would be to weight the algorithm to account for institutionalized sexism, racism, ableism, and economic inequality.
	Finally, O’Neil outlined the model for teacher evaluations that was unjust and, frankly, stupid. The algorithm was based off the test scores of students, which we can all agree is not an accurate reflection of the quality of the teacher. I knew people in middle school who filled in random bubbles on standardized tests to be funny. This has very serious consequences, as good teachers were fired and schools lost funding as a result of the model. The ripple effects of having good teachers and good schools, had the model not been relied on, would have been far reaching. Furthermore, the government wouldn’t release the code that was used to determine who was good and who was bad! This model is still being used, as schools that perform poorly on tests receive less funding than schools that perform well. This model ignores factors such as class, race, and special needs and ESL students. It is not a just model, but it is being used to keep disadvantaged students disadvantaged. Good thing we can count on Betsy DeVos!
