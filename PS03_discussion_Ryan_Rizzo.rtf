{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww13840\viewh11160\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 In terms of crime recidivism, there are algorithms in place in the American justice system that cause biases to enter into decisions on sentencing.  For example, judges use an algorithm which predicts the likelihood of the criminal committing another crime.  This algorithm "score" is given to the judge before sentencing.  Judges tend to give greater length-sentences to criminals who have higher likelihood of committing another crime.  However, this algorithm is somewhat controversial due to its secrecy and self-perpetuity.  This is a common theme across all these biased algorithms that Cathy O'Neil speaks of.  Sure, the scoring system may be mathematically accurate (maybe), but the interpretation of these algorithms is where their use becomes iffy.  The data that is being analyzed in the first place is biased itself due to this self-perpetuity.  The algorithms can only be as good as the data which fuels them, and because this data has been biased by these algorithms in the first place, the data is a big issue to be aware of.  But Cathy doesn't believe people are aware of this data bias.\
\
In tech, the hiring of software engineers can have major biases.  There are tons of resumes to sift through, and one maybe accurate but certainly biased mechanism for sorting applications is to filter by school.  There are much more complicated algorithms incorporating deep learning and neural networks for the hiring process and applicant's data ad GPA's, etc. but in the end, regardless of the fanciness of the model, they all have inherent biases.  Algorithms are based off finding new examples of old success stories, and in order to find the patterns of applicants who can get there, you are eliminating people who may follow a different path to their sucess as engineers.  But these people get filtered out just because the data didn't show a similar success story.  Also, due to the data being biased by these algorithms in the first place, the algorithms cannot make new, moral, or innovative decisions regarding hirings.  It only can keep doing the same things it has done in the past due to the biased data.\
\
Lastly, with evaluating teachers, there was an algorithm which judged teachers by performance on standardized testing.  Poverty is correlated with bad performance on standardized testing, so teachers are punished for teaching poor kids.  Therefore, there is incentive for a teacher to want to teach richer children.  Even when a "better" algorithm was used, which evaluates expected scores for each student and grades the teacher on their student's performance relative to their predicted performance, it has its own failings.  Teachers are being held accountable for the error terms on a bad algorithm.  It was shown there was little to no repeatability of the scoring system year to year.  One teacher went from scoring a 6/100 to a 96/100, which can either be looked at as unreliability of the algorithm, or the potential motivation aspect of the algorithm for teachers.  If the latter, that is great, but data seems to show it is more likely to be the former.  The model simply has a ton of noise and not much signal.}