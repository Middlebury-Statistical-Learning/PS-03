In Cathy O'Neil's article, she points out how the crime recidivism reinforce the inequality, based on race and class of offenders. According to O'Neil, this model scores people into a low, middle, or high risk category. This scoring is based on the number of interactions that a person has with police officers, include low risk interactions such as marijuana posession. Additionally, this model incorporates a questionare that asks things such as, "Are you from a high risk area?" and "Have any of your relatives also had interactions with the law?" Although these variables are not necessarily causal, the "high risk" individuals are sentenced to longer jail sentences. These longer sentences increase the chances of a parolee becoming a repeat offender. This association then make the variable for high risk individuals seem to be causal. 

For the technology example, O'Neil states that the hypothetical tech firm uses previous "successful" hires in order to sort applications more "efficiently" so that the applications considered are only from the most "qualified" candidates. However, this model can ignore certain biases within the work force. Specifically, the application filter for the hypothetical company is that "successful" applicants are based on a minimum of 3 years of employment in the firm, and at least 2 promotions in those 3 years. However, this model totally ignores the fact that women are a minority in tech companies, meaning that the number of women applying to the tech company is a lot smaller than the number of men. As a result, the number of successful women in the company is a lot smaller than men, so women are weeded out of the model. The same is also true for firms that hire predominately from the same universities. Since these universities are represented way more often than others, applicants from these universities will be ranked as more successful than applicants from other universities. In order to use machine learning to sift through applications, we must have a very good model. 

For the teacher evaluation example, O'Neil already notes how the system is flawed. Originally, teachers were rated based on how many of their students were proficient by the end of year. Those teachers with bad scores would be removed from the system (i.e. fired). However, this system targeted teachers working at lower income students, whose students were less likely to be preficient by the end of the year. As a result, this model artificially  singled out teachers who were working at poor schools as opposed to rich schools. To "fix" the model, an individual predicted score was calculated for each student, based on their previous score, and other factors such as income, which can be indirectly determined by certain factors, such as the inclusion in the Free and Reduced Cost School Lunch Program. However, this system also has its flaws. For example, if a teacher cheats for their students on examples, or is really good at teaching to exams, this can give students and unusually high predicted score for the next year. When this high score is not achieved next year, this translates into meaning that the current teacher is really bad. This means that extremely flawed data can be used to determine the effect in high rish situations. 

